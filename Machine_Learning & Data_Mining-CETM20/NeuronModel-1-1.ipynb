{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Neurons (Biological + MCP Neuron + Python)\n",
    "\n",
    "## MCP Neuron\n",
    "\n",
    "We can define the MCP Neuron as below:\n",
    "\n",
    "$$x = [x_{1}, x_{2}, \\dots, x_{n}]; x_{i} \\in \\{0,1\\}$$\n",
    "$$w = [w_{1}, w_{2}, \\dots, w_{n}]; w_{i} \\in \\{-1,0,1\\}$$\n",
    "\n",
    "$$\\hat{y} = x \\cdot w \\geq \\theta$$\n",
    "\n",
    "Where we use a weight of 1 as *excitatory* and a weight of -1 as an *inhibitory* neural connection. Our inputs are either, 0, or 1 (binary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing pre-requisite external Libraries: Numpy\n",
    "Jupyter notebooks allow us to use magic commands\n",
    "    `pip install numpy`\n",
    "From a command prompt, or we can execute it here using %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Imports go at the top of our solution files\n",
    "\n",
    "# Neuron Model\n",
    "# Define some inputs\n",
    "x = np.array([ 1, 0 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR [1 0]: True\n"
     ]
    }
   ],
   "source": [
    "# OR function\n",
    "# If either input is 1, the output is 1.\n",
    "\n",
    "w = np.array([ 1, 1 ]) # Listen to both input\n",
    "neuron_output = np.dot( x, w )\n",
    "activation_threshold = 1\n",
    "print(f\"OR {x}: {neuron_output >= activation_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND [1 0]: False\n"
     ]
    }
   ],
   "source": [
    "# AND function\n",
    "# Only output 1 if both inputs are 1\n",
    "\n",
    "w = np.array([ 1, 1 ]) # Listen to both input\n",
    "neuron_output = np.dot( x, w )\n",
    "activation_threshold = 2 # Both need to be high.\n",
    "print(f\"AND {x}: {neuron_output >= activation_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOR Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOR [1 0]: False\n"
     ]
    }
   ],
   "source": [
    "# NOR \n",
    "# Only activate when both inputs are 0.\n",
    "\n",
    "w = np.array([ -1, -1 ]) # Invert the inputs.\n",
    "neuron_output = np.dot( x, w )\n",
    "activation_threshold = 0\n",
    "print(f\"NOR {x}: {neuron_output >= activation_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excite, Inhibit, and Ignore: Simple Neuron Example\n",
    "\n",
    "- **Excite**: Weight = 1 (input helps neuron fire)\n",
    "- **Inhibit**: Weight = -1 (input prevents neuron from firing)\n",
    "- **Ignore**: Weight = 0 (input has no effect)\n",
    "\n",
    "Below, see how each logic affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [1 1 1]\n",
      "Weights: [ 1 -1  0] (Excite, Inhibit, Ignore)\n",
      "Dot product (x·w): 0\n",
      "Neuron fires (output >= 1): False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example input: three features\n",
    "x = np.array([1, 1, 1])\n",
    "\n",
    "# Excite, Inhibit, Ignore\n",
    "w = np.array([1, -1, 0])  # Excite first, inhibit second, ignore third\n",
    "\n",
    "neuron_output = np.dot(x, w)\n",
    "activation_threshold = 1\n",
    "fires = neuron_output >= activation_threshold\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Weights: {w} (Excite, Inhibit, Ignore)\")\n",
    "print(f\"Dot product (x·w): {neuron_output}\")\n",
    "print(f\"Neuron fires (output >= {activation_threshold}): {fires}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied Example - Should I eat that?\n",
    "Image we are a bird, and wondering what qualities make an object safe to eat.\n",
    "Below we have a few types of object, whether they are purple, round, and bouncy. Depending on this, we should eat this item.\n",
    "\n",
    "| Object | Purple | Round | Bouncy | Eat? |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Blueberry | Yes | Yes | No | Yes |\n",
    "| Golf Ball | No | Yes | Yes | No |\n",
    "| Violet | Yes | No | No | No |\n",
    "| Hot Dog | No | No | No | No |\n",
    "\n",
    "At the moment, this is in a Yes/No format, which isn't any use. We can only do arithmetic with numbers. Let's represent them that way.\n",
    "\n",
    "Numerically:\n",
    "\n",
    "| Object | Purple | Round | Bouncy | Eat? |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Blueberry | 1 | 1 | 0 | 1 |\n",
    "| Golf Ball | 0 | 1 | 1 | 0 |\n",
    "| Violet | 1 | 0 | 0 | 0 |\n",
    "| Hot Dog | 0 | 0 | 0 | 0 |\n",
    "\n",
    "We should separate out our inputs, from the target we wish to output. In this scenario, the decision to 'eat' should be the output of our neuronal model.\n",
    "\n",
    "Input:\n",
    "\n",
    "|  Purple | Round | Bouncy |\n",
    "| --- | --- | --- |\n",
    "| 1 | 1 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 0 |\n",
    "| 0 | 0 | 0 |\n",
    "\n",
    "Target:\n",
    "\n",
    "| Eat |\n",
    "| --- |\n",
    "| 1 | \n",
    "| 0 |\n",
    "| 0 |\n",
    "| 0 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment\n",
    "Below you may see some familiar code. This is exactly the same as what we have been doing thus far, except our input and weight vectors have 3 components instead of 2.\n",
    "These correspond to our 'features' data (columns). Purple, Round, and Bouncy, in that order. Order here is very important.\n",
    "\n",
    "You should try changing the inputs. E.g `[1, 1, 1]` would be a Purple, Round, and Bouncy object. From our original table, we don't have an object which has all three of these properties.\n",
    "`[1, 1, 0]` would be Purple, Round but NOT Bouncy. This is a blueberry. We should eat blueberries.\n",
    "\n",
    "At the moment, our weight vector has all 0s. Our neuron isn't listening to any of our inputs.\n",
    "\n",
    "Try changing these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 1, 1]) # Purple, Round, Bouncy.\n",
    "\n",
    "# TODO: Populate these appropriately\n",
    "weights_w = np.array([0, 0, 0])\n",
    "threshold = 0\n",
    "should_eat_neuron_output = np.dot(x, weights_w)\n",
    "print(int(should_eat_neuron_output >= threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have some code which will go through all possible permutations of our inputs, ranging from `[0,0,0]` to `[1,1,1]`.\n",
    "Using the Object table we defiend above, can you figure out which object 'features' need to be listened to produce the correct output? Secondly, what would the threshold need to be for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0 0], Raw Output: 0\t Fire: 1\n",
      "Input: [0 0 1], Raw Output: 0\t Fire: 1\n",
      "Input: [0 1 0], Raw Output: 0\t Fire: 1\n",
      "Input: [1 0 0], Raw Output: 0\t Fire: 1\n",
      "Input: [1 1 0], Raw Output: 0\t Fire: 1\n",
      "Input: [0 1 1], Raw Output: 0\t Fire: 1\n",
      "Input: [1 1 1], Raw Output: 0\t Fire: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_inputs_to_test = [\n",
    "    np.array([0, 0, 0]),\n",
    "    np.array([0, 0, 1]),\n",
    "    np.array([0, 1, 0]),\n",
    "    np.array([1, 0, 0]),\n",
    "    np.array([1, 1, 0]),\n",
    "    np.array([0, 1, 1]),\n",
    "    np.array([1, 1, 1]),\n",
    "]\n",
    "\n",
    "weights_w = np.array([0, 0, 0])\n",
    "threshold = 0\n",
    "for x in all_inputs_to_test:\n",
    "    should_eat_neuron_output = np.dot(x, weights_w)\n",
    "    print(f\"Input: {x}, Raw Output: {should_eat_neuron_output}\\t Fire: {int(should_eat_neuron_output >= threshold)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron (Weights + Bias w/ Activation Function)\n",
    "\n",
    "We can model the bias of a perceptron; the push towards (or away) from a given threshold. We are no longer constrained by the binary inputs of the MCP Neuron, we can now use any Real for the inputs; likewise, for our weights.\n",
    "\n",
    "E.g `x = [-1.4, 3.7, 1.0]`, `w = [5, 1.2, 2.4, 3.6]`\n",
    "\n",
    "We can therefore expand our Neuron Model definition thus:\n",
    "$$x = [x_{1}, x_{2}, \\dots, x_{n}]; x_{i} \\in \\mathbb{R}$$\n",
    "$$w = [w_{1}, w_{2}, \\dots, w_{n}]; w_{i} \\in \\mathbb{R}$$\n",
    "\n",
    "$$\\hat{y} = x \\cdot w + b \\geq \\theta$$\n",
    "\n",
    "By the definition of a perceptron we also have an *activation function*; our final definition is thus:\n",
    "$$\\hat{y} = f(x \\cdot w + b)$$\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad x \\geq \\theta \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "\\end{equation*}\n",
    "\n",
    "This is a fanciful way of saying that the neuron will fire if the weighted inputs, plus the bias exceed our activation threshold, otherwise it will stay dormant.\n",
    "In reality, we can change this function `f(x)`, but for now we will keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3 1.  0.6]\n",
      "Neuron Output: 0.26000000000000006; Activated:\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Neuron Model\n",
    "# Define some inputs\n",
    "x = np.array([ 2.3, 1.0, 0.6 ])\n",
    "\n",
    "# Our inputs to the model would be 2.3, 1.0, and 0.6.\n",
    "print(x)\n",
    "\n",
    "\n",
    "bias = 0.5\n",
    "w = np.array([ -0.4, 0.5, 0.3 ])\n",
    "neuron_output = np.dot( x, w ) + bias\n",
    "activation_thresh = 1\n",
    "print(f\"Neuron Output: {neuron_output}; Activated:\\t{neuron_output >= activation_thresh}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Bias\n",
    "Currently our Neuron output is 0.26. This is below our activation threshold, and therefore we don't fire.\n",
    "Change the bias amount so that the neuron will fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron Output high bias: 1.0; Activated:\tTrue\n"
     ]
    }
   ],
   "source": [
    "# TODO: Change the Bias to make the function activate.\n",
    "bias = 1.24\n",
    "\n",
    "# Recalculate Neuron Output.\n",
    "neuron_output = np.dot( x, w ) + bias\n",
    "print(f\"Neuron Output high bias: {neuron_output}; Activated:\\t{neuron_output >= activation_thresh}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have modified our bias, let's see what happens to the behaviour of our Neuron. Let's put all our inputs to 0, and see the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron Output without inputs: 1.24; Activated:\tTrue\n"
     ]
    }
   ],
   "source": [
    "# TODO: What happens if we just remove all inputs?\n",
    "x_blank_inputs = np.array([ 0, 0, 0]) # No inputs activated.\n",
    "\n",
    "neuron_output = np.dot( x_blank_inputs, w ) + bias\n",
    "print(f\"Neuron Output without inputs: {neuron_output}; Activated:\\t{neuron_output >= activation_thresh}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neuron is now ALWAYS ON! As our weights vector is `[ -0.4, 0.5, 0.3 ]`, only a sufficiently strong $x_{1}$ can turn it off again.\n",
    "\n",
    "Tweak the inputs to the model to cause the neuron to turn off again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron Output gamed inputs: -0.04000000000000026; Activated:\tFalse\n"
     ]
    }
   ],
   "source": [
    "# TODO: Change the x input values (leave bias alone) to make the Neuron deactivate.\n",
    "x_tweaked_inputs = np.array([3.2, 0, 0])\n",
    "neuron_output = np.dot( x_tweaked_inputs, w ) + bias\n",
    "print(f\"Neuron Output gamed inputs: {neuron_output}; Activated:\\t{neuron_output >= activation_thresh}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the Threshold (Bias) Learned in Machine Learning?\n",
    "\n",
    "In machine learning, the threshold is not set manually. Instead, it is learned from the data during training. This threshold is often called the **bias**.\n",
    "\n",
    "- The model starts with random weights and bias.\n",
    "- During training, it makes predictions and compares them to the true answers.\n",
    "- It then adjusts both the weights and the bias to reduce the prediction error, using an optimization algorithm (like gradient descent).\n",
    "- This process continues until the model finds the weights and bias that best fit the data.\n",
    "\n",
    "**Summary:**\n",
    "- The bias (threshold) is a parameter that is automatically updated during training, just like the weights.\n",
    "- This allows the neuron to learn the best decision boundary for the data, rather than relying on a fixed, hand-picked threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Practice Tasks\n",
    "\n",
    "Try these tasks to deepen your understanding of neuron models and logic with numpy:\n",
    "\n",
    "1. **Design a Custom Logic Gate:**  \n",
    "   Create a neuron (using numpy) that implements the XOR logic gate for two binary inputs. Show your weights, threshold, and test all input combinations.\n",
    "\n",
    "2. **Multi-Feature Neuron:**  \n",
    "   Given four binary features, design a neuron that only fires if exactly two of the inputs are 1. Show your weights, threshold, and test all possible input combinations.\n",
    "\n",
    "3. **Visualize Decision Boundaries:**  \n",
    "   For a neuron with two real-valued inputs, randomly generate 100 (x1, x2) pairs in [0, 1]. Assign weights and a threshold so that the neuron fires for points above the line x1 + x2 = 1. Plot the points, coloring them by whether the neuron fires.\n",
    "\n",
    "4. **Bias Experiment:**  \n",
    "   Take a neuron with three inputs and random weights. Systematically vary the bias from -5 to 5 in steps of 1. For a fixed input, plot the neuron output as a function of the bias.\n",
    "\n",
    "5. **Inhibitory/Excitatory Mix:**  \n",
    "   Create a neuron with five inputs: two excitatory, two inhibitory, and one ignored. Randomly generate 10 input vectors of 0s and 1s. For each, print the input, the weighted sum, and whether the neuron fires.\n",
    "\n",
    "_Try to solve these without hints!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, going through the materials really gave me a lot of appreciation for how the human brain has been studied and simulated in the application of artificial learning.\n",
    "\n",
    "Going from the biology breakdown of how the human-child brain works, through ways that cannot be explicitly programmed. We learn through our senses, image and voice recognition,  etc, all of which cannot be explicitly programmed to be learnt artificially. For example, children learn to recognize colours through constant image reinforcement and not necessarily from any attributes. Hence, explicitly programming attributes may be futile or difficult to document, and enabling constant self-recognition and self-learning was how artificial learning evolved.\n",
    "\n",
    "As illustrated in the article from last week's read, the connectionism is simply with the diagram where Soma is the core part of the neuron that both stores and processes information (or newly learnt concepts),  while the Dendrites receive signals from other Somas through the Synapses. The 'Synapses' are at the tail end of each 'Axon' of the brain neuron. Received information is then processed by the Soma, and the output is sent to other neurons through the Axon. The more practice we put into learning, the \"stronger the weight\" of the axon becomes, and vice versa.  This is the biological summary.\n",
    "\n",
    "In the engineering framework, the received information (via the dendrites) is the input [in our coding note variable x], and the axon that sends information is a representation of the weight. As explained above, the more practice we put into learning, the stronger the weight of the axon.\n",
    "\n",
    "For every new concept learnt, the axon connected to that neuron is only strengthened by how much interaction and connectivity the entire neuron (soma, dendrites, synapses, and axon) has with other neurons. This interaction (Weight x Input + Bias) increases the likelihood that our Neuron Fire will be activated.\n",
    "\n",
    "This concept was conceived by McCulluch Pitts, a neurophysiologist, in 1943, but there wasn't much development. \n",
    "\n",
    "The next development on this concept was by Frank Resenblatt, who developed the first learning algorithm in 1958. It was the first learning algorithm because of the feedback loop embedded in this development. It basically made old weights revise themselves based on the learning rate, error, and input. Which is a perfect simulation of how the human brain learns- we strengthen our knowledge (new weights) by building on what we already know (old weights), learning more (inputs), failing and keep trying (error), and by consistently learning (learning rate); New_weights = Old_weights + (Learning_rate x Error x Input). The artificial simulation is called the perceptron, which is similar to the core part of the neuron, called the soma. However, the downside to the Perceptron was the fact that the logic gates were linear (AND, OR, NOT...). Meaning, the algorithm could only be either a cat or not a cat. This left the gap for hidden layers. \n",
    "\n",
    "This gap birthed Deep learning, which gave room for not just the introduction of hidden layers (more non-linear logic gates (XOR, XAND, NOR, NAND)  but also backpropagation (the ability of the algorithm to backtrace the error from each input layer and then adjust each weight to minimize errors)\n",
    "\n",
    "In general, I see that we circled back to logic gates (from the previous course)- which I'm excited about! The code exercise was good (plan to take up further personal exercises). I understand the threshold value is set by the logic being used (as per the MCP logic gates). However, I read that the value of the bias may depend on the data and not an arbitrary value- I'm not sure how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_DM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
