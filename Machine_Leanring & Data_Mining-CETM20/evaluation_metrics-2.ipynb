{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "We will need the library **scikit-learn** for this example. Ensure this is installed by executing the code cell below or by using pip manually in the command prompt / terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\kojus\\anaconda3\\envs\\ml_dm\\lib\\site-packages (1.7.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\kojus\\anaconda3\\envs\\ml_dm\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\kojus\\anaconda3\\envs\\ml_dm\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kojus\\anaconda3\\envs\\ml_dm\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kojus\\anaconda3\\envs\\ml_dm\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "This week, we've looked at training our Perceptron model, and then how we can construct bigger networks by layering these perceptrons together. Today we will introduce some evaluative metrics for our networks when we wish to evaluate performance for a binary classification problem (falling under supervised machine learning).\n",
    "\n",
    "Before we begin, let us assume that you have created a model in Keras, and it is **trained**.\n",
    "\n",
    "We should have the following:\n",
    "* $X$ - Our testing dataset\n",
    "$$X = (x_{1}, x_{2}, \\dots, x_{n}) $$\n",
    "Where $x_{i}$ is a single record, containing data. E.g could be 11 features of wine (acidity, ph, sulphates, etc). This will *NOT* have the y label in it.\n",
    "\n",
    "* $y_{true}$ - Labels corresponding to our data.\n",
    "$$y_{true} = (y_{1}, y_{2}, \\dots, y_{n})$$\n",
    "\n",
    "* $model$ - A trained Keras model\n",
    "\n",
    "For each entry in $X$ we have a corresponding entry in $y_{true}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining predictions\n",
    "\n",
    "In order to evaluate how well our model has done in the task, we must first obtain what the model thinks the correct y labels should be. We can do this using the `model.predict()` function. This provides input data to the model and will run a forward pass on the data to obtain the network's estimate. This is called *inference*. Once we have all the predicted y values, we can run our own metric calculations.\n",
    "\n",
    "E.g\n",
    "```python\n",
    "y_pred = model.predict( X )\n",
    "```\n",
    "\n",
    "Typically, we save the output of this as $y_{pred}$ to differentiate from our ground-truth data $y_{true}$.\n",
    "\n",
    "For binary classification, let's provide some example data for what $y_{true}$ and $y_{pred}$ may look like.\n",
    "\n",
    "$y_{true} = [1, 1, 1, 0, 0, 1, 0, 1]$\n",
    "\n",
    "$y_{pred} = [1, 1, 0, 0, 1, 0, 0, 1]$\n",
    "\n",
    "Looking at the two lists, we can see that mostly our network predicted correctly; however, some cases the network gets the answer wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP, TN, FP, FN\n",
    "\n",
    "We can denote 4 basic metrics for classification evaluation. These are true-positive (TP), true-negative (TN), false-positive (FP), and false-negative (FN). We treat these as tallies, and count up for all elements of our predicted output.\n",
    "\n",
    "* TP - Where the ground-truth class is positive (1) and the predicted class is positive (1)\n",
    "* TN - Where the ground-truth class is negative (0) and the predicted class is negative (0)\n",
    "* FP - Where the ground-truth class is negative (0) and the predicted class is positive (1) - I.e Falsely predicting positive\n",
    "* FN - Where the ground-truth class is positive (1) and the predicted class is negative (0) - I.e Falsely predicting negative\n",
    "\n",
    "Let's go through our example, and calculate these:\n",
    "\n",
    "* TP: How many times is there a 1 for both ground-truth and predicted? **3 times**. (Indices 0, 1, and 7)\n",
    "* TN: How many times are both negative? **2 times**. (Indices 3 and 6)\n",
    "* FP: Predict positive, when it should be negative? **Once**. (Index 4)\n",
    "* FN: How many times falsely negative? **2 times** (Indices 2 and 5)\n",
    "\n",
    "Note: These tallies should all add up to equal the number of examples we have. 3 + 2 + 1 + 2 = 8. The length of $y_{pred}$ (and $y_{true}$) equals 8 too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "Now we have our four base metrics, we can put these into a **confusion matrix**. This helps us visualise how many times our classes get 'confused'; that is where a 1 should be a 0, or a 0 should be 1. This uses a heatmap approach to help us identify at-a-glance where we may need to improve.\n",
    "\n",
    "For now, we will use some dummy data for our $y_{true}$ and $y_{pred}$. Later this week we will plug in some real model output.\n",
    "\n",
    "Documentation: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix # Import the confusion matrix from sklearn\n",
    "from sklearn.metrics import ConfusionMatrixDisplay # This is for the visual bit\n",
    "\n",
    "y_true = [1, 1, 1, 0, 0, 1, 0, 1]\n",
    "y_pred = [1, 1, 0, 0, 1, 0, 0, 1]\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D array returned, but not very useful unless we know which indices are which."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 3\n",
      "FP: 1\n",
      "TN: 2\n",
      "FN: 2\n"
     ]
    }
   ],
   "source": [
    "# We can even assign variables to the output! adding .ravel() to the conf matrix.\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(f\"TP: {tp}\")\n",
    "print(f\"FP: {fp}\")\n",
    "print(f\"TN: {tn}\")\n",
    "print(f\"FN: {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the matrix makes things significantly easier.\n",
    "\n",
    "Note: Each row is a class in the ground-truth data. Each column is a class in the predicted. Therefore, X-Axis is predicted, Y-axis is groundtruth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "ConfusionMatrixDisplay.from_predictions requires matplotlib. You can install matplotlib with `pip install matplotlib`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kojus\\anaconda3\\envs\\ML_DM\\lib\\site-packages\\sklearn\\utils\\_optional_dependencies.py:17\u001b[0m, in \u001b[0;36mcheck_matplotlib_support\u001b[1;34m(caller_name)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mConfusionMatrixDisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kojus\\anaconda3\\envs\\ML_DM\\lib\\site-packages\\sklearn\\metrics\\_plot\\confusion_matrix.py:472\u001b[0m, in \u001b[0;36mConfusionMatrixDisplay.from_predictions\u001b[1;34m(cls, y_true, y_pred, labels, sample_weight, normalize, display_labels, include_values, xticks_rotation, values_format, cmap, ax, colorbar, im_kw, text_kw)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_predictions\u001b[39m(\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    370\u001b[0m     text_kw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    371\u001b[0m ):\n\u001b[0;32m    372\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot Confusion Matrix given true and predicted labels.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    For general information regarding `scikit-learn` visualization tools, see\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;03m    >>> plt.show()\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     \u001b[43mcheck_matplotlib_support\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.from_predictions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m display_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kojus\\anaconda3\\envs\\ML_DM\\lib\\site-packages\\sklearn\\utils\\_optional_dependencies.py:19\u001b[0m, in \u001b[0;36mcheck_matplotlib_support\u001b[1;34m(caller_name)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m requires matplotlib. You can install matplotlib with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install matplotlib`\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(caller_name)\n\u001b[0;32m     22\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: ConfusionMatrixDisplay.from_predictions requires matplotlib. You can install matplotlib with `pip install matplotlib`"
     ]
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some more data to make this more apparant. At the moment, our true-negative score is the same as our false-negative score, so this classifier wouldn't be too useful.\n",
    "**Ideally**, we want the upper left, and bottom right to be high numbers, and the opposing diagonal to be as close to 0 as possible. This means no confusion, and full true-positive, and true-negative tallies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f07c42051c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true = [1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0]\n",
    "y_pred = [1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0]\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derived Metrics\n",
    "\n",
    "From these basic 4 tallies we can define some additional metrics which can be useful.\n",
    "\n",
    "* Accuracy - How many did we guess correctly, out of all possible entries?\n",
    "$$Accuracy = \\frac{TP\\ +\\ TN}{TP\\ +\\ TN\\ +\\ FP\\ +\\ FN}$$\n",
    "\n",
    "* Specificity ( True Negative Rate ) - Of all the negative cases, how many did we get right?\n",
    "$$ Spec = \\frac{TN}{TN + FP} = \\frac{TN}{All\\ groundtruth\\ negatives} $$\n",
    "\n",
    "* Sensitivity (Recall / True Positive Rate ) - Of all the positive cases, how many did we get right?\n",
    "$$ Sens = \\frac{TP}{TP + FN} = \\frac{TP}{All\\ groundtruth\\ positives} $$\n",
    "\n",
    "* Precision (Positive Predictive Value) - If we guess 1 for everything, our sensitivity will be 100%, but we're not very precise. This answers the question \"How many positive predictons were actually correct?\"\n",
    "$$ Precision = \\frac{TP}{TP\\ +\\ FP} = \\frac{TP}{All\\ predicted\\ positives} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balance. Which to choose? Introducing F1-Score\n",
    "\n",
    "Depending on the task, you may wish to focus on maximising a given evaluative metric. You may care more about catching all positive cases, even at the risk of over predicting.\n",
    "\n",
    "An example may be cancer diagnoses. It can be argued that it's better to provide a false cancer diagnosis (FP) than to miss somebody who may need medical intervention (FN). This would focus on high sensitivity at the cost of precision.\n",
    "\n",
    "In most real world cases, we want to balance out these considerations to obtain an overall picture of how well our classify performs. This can be achieved by something called the **F1-Score**.\n",
    "\n",
    "$$ F1-Score = \\frac{2}{\\frac{1}{precision} \\frac{1}{recall}}  = 2\\frac{precision * recall}{precision + recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate for the long examples.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Just seeing what we've got.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43my_true\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_pred)\n\u001b[0;32m      7\u001b[0m tn, fp, fn, tp \u001b[38;5;241m=\u001b[39m confusion_matrix(y_true, y_pred)\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate for the long examples.\n",
    "\n",
    "# Just seeing what we've got.\n",
    "print(\"y_true\", y_true)\n",
    "print(\"y_pred\", y_pred)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "print(f\"TP: {tp}\")\n",
    "print(f\"FP: {fp}\")\n",
    "print(f\"TN: {tn}\")\n",
    "print(f\"FN: {fn}\")\n",
    "\n",
    "acc = (tp + tn) / (tp+fp+tn+fn)\n",
    "spec = tn / (tn+fp)\n",
    "sens = tp / (tp+fn)\n",
    "prec = tp / (tp+fp)\n",
    "\n",
    "f1 = 2 * ((prec * sens)/(prec + sens))\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Specificity: {spec}\")\n",
    "print(f\"Sensitivity: {sens}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"F1-Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `sklearn` utilities to calculate many of these for us, directly from $y_{true}$ and $y_{pred}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SKLEARN.\n",
      "Accuracy: 0.8\n",
      "No utility for specificity alone\n",
      "Sensitivity: 0.8\n",
      "Precision: 0.7619047619047619\n",
      "F1-Score: 0.7804878048780488\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "print(\"Using SKLEARN.\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "print(f\"No utility for specificity alone\")\n",
    "print(f\"Sensitivity: {recall_score(y_true, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_true, y_pred)}\")\n",
    "print(f\"F1-Score: {f1_score(y_true, y_pred)}\")\n",
    "\n",
    "# See:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.82        25\n",
      "           1       0.76      0.80      0.78        20\n",
      "\n",
      "    accuracy                           0.80        45\n",
      "   macro avg       0.80      0.80      0.80        45\n",
      "weighted avg       0.80      0.80      0.80        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# See: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
    "# A 'kitchen sink' utility for gathering precision, recall, f-1, support, accuracy.\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_DM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
